We compare the different parallel tree search algorithms (Lazy_smp, ABDADA and Simple-ABDADA) on a few different metrics and under different constraints. The default test setting is searching the start position up to a depth of 10 half moves (plus q-search at leaves). The default memory used for the transposition table is 16 GB. The system has 12 cores and 24 hyperthreads.

The baseline_testing folder contains a few plots: the easiest to interpret plot is the speedup in terms of nodes per second searched. The plot at "./baseline_testing/plots/pos1_16384_abdada_d10_nps.pdf" shows the average nps speedup by search depth, from depth 5 to 10. Unsurprisingly, for a low search depth of 5, the speedup is not very good and even declines once the number of threads exceeds the number of cores. This is due to the very short search time for which the thread creation overhead outweighs the parallelism speedup. However, for search depths 8 or more, the speedup in terms of nodes per second is almost linear up to 12 cores, and then keeps rising on a flatter linear trajectory once the algorithm uses more threads than cores, all the way to 24 threads. This behavior is consistent across almost all tests and for all the algorithms, since they are all loosely synchronized and the only locking is in rarely contested spinlocks in the different hash table buckets.

The perhaps more interesting benchmark looks at the speedup in terms of time spent to reach that depth 10. This speedup is considerably lower than the pure nps speedup and is also quite erratic across different search depths. (the latter may be in part due to the randomization in the selection of the order in which nodes are computed) 
The reason for this is that due to the loosely synchronized nature of the algorithms, there are two inefficiencies: the same node may be evaluated by multiple threads. Also, due to the imperfect knowledge of the computation of the other threads, subtrees may be evaluated with weaker upper and lower bounds than they would in a sequential run, leading to more nodes being needed to evaluate that subtree. This is known as search overhead and is plotted in the plot "pos1_16384_abdada_d10_so.pdf". Apart from one outlier search depth, the search overhead for the ABDADA based algorithms is considerably lower than that of the naive parallelization algorithm.
Finally, the comparison plots compare the speedups for depth 10 searches across the different algorithms. The nps speedup is very similar between the algorithms, showing that the slowdown incurred from the additional ABDADA logic is quite small. The lower search overhead of those algorithms therefore gives them a much higher time speedup.

The next set of plots concern the tuning of a hyper parameter: defer depth. This parameter determines how far away from leaves the synchronization and work splitting between the threads is used. In "defer_depth_tuning/plots/abdada_defer_depth_nps.pdf" we see that a higher defer depth is slightly faster in terms of nps. This is to be expected since even at a small cost, synchronization just one level above the leaves does get incurred in a lot more nodes. In terms of time speedup the plot is less clear and again much more erratic. A higher defer depth might lead to slightly more search overhead, as small subtrees won't be split up between the threads. For all the other tests, a defer depth of 3 was used.

Another consideration is the amount of available memory. We compare the 16 GB used with a test run that uses 1 GB and one that uses just 64 MB of memory. This limited memory does not affect the speed in terms of nodes per second, however, it will lead to a more overloaded transposition table that will have to discard many entries. However, this affects the search both in single as well as multi-threaded execution, so it is not obvious whether the parallelism itself is helped or hurt by less memory. For the naive algorithm, the high memory version appears to yield a slightly lower speedup. For the ABDADA-based algorithms, the reduced memory surprisingly improved performance and speedup. More research may be needed to explore the effects of a smaller transposition table in such a scenario.

Furthermore, the shape of the tree may impact the performance of the search algorithms. We compare three positions: Position 1 is the starting position, which has a medium average branching factor. Position 2 is a middle game position which has a high average branching factor. Finally, position 3 is an endgame position with a low average branching factor. In the first two positions the nps speedup is the expected linear curve. However, somewhat surprisingly, in position 3 the naive parallelization suffers a drop in nps past a certain depth and thus does not show the same linear speedup.
This can also be seen in the time speedup where the ABDADA based algorithms perform much better than the naive algorithm, conversely, less so in the high branching factor position where the time spent is much more erratic. So at least for this simple program, a low branching factor appears to work well with ABDADA, whereas at a high branching factor it has a smaller benefit.

The above tests were all performed using a simple but reasonable evaluation function at leaves. This should in theory give a stable search tree and evaluation of subtrees across different search depths, as a reasonable evaluation function should give a result on a leaf similar to that of an actual search starting at that leaf. But such a good evaluation function may not always be available. To gauge the effectiveness of the search algorithms in that environment, we try a different evaluation function, namely a pseudo-random one. The results show that the ABDADA-based algorithms perform worse than for the sensible evaluation function, while the naive parallel algorithm appears to be largely unaffected. This is likely due to the lower stability of the search tree making the "distributing" of work more difficult / less efficient, which is not a concern for the naive algorithm. However, even so the naive algorithm is still outperformed.

Overall it can be seen that the naive parallelization is outperformed by the more sophisticated ABDADA based strategies. The two different implementations of the ABDADA principle perform quite similarly, the simple ABDADA may be marginally faster, however, more data would be needed to see any significant difference. It does however have the advantage of a simpler and cleaner implementation which may allow for easier further performance improvements.
It should be noted that the search used contains almost no search heuristics, leading to a high uncertainty in time and nodes needed to search to a certain depth, as the order of node expansion can greatly impact the algorithms performance and in this case was simply done randomly. A smarter search may show different results, in fact most competitive chess engines appear to use a version of the naive search algorithm rather than the more sophisticated ABDADA-based ones.
